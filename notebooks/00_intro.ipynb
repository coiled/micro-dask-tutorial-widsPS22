{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b6fe04e-fde7-4823-8ac5-d230b784ce7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"../intro-slide-wids22.svg\"\n",
    "     max_width=100%\n",
    "     height=auto\n",
    "     alt=\"WiDS title slide\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414d9eeb-6de9-40e6-a59b-0601c254f40c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center>\n",
    "  <img src=\"https://raw.githubusercontent.com/dask/marketing/main/source/images/dask-horizontal.svg\" alt=\"Dask logo\" width=55%/>\n",
    "</center>\n",
    "\n",
    "# Materials and setup\n",
    "\n",
    "Tutorial materials available at https://github.com/coiled/micro-dask-tutorial-widsPS22. Two ways to go through the tutorial:\n",
    "- Run using Binder (no setup required, recommended for today!)\n",
    "- Run locally on your laptop (recommended for going back through materials later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d7de6e-7235-4422-b346-dfaa5a9a3801",
   "metadata": {},
   "source": [
    "# A common problem\n",
    "\n",
    "Timseries dataset for one week, with one row per second (publicly accessible, see [this github repo](https://github.com/coiled/coiled-datasets) for more background). Using pandas, it's very easy to take one week of this dataset and calculate the counts of unique values for a given column:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_parquet(\"s3://coiled-datasets/timeseries/20-years/parquet/part.0.parquet\")\n",
    "df['name'].value_counts().astype(int)\n",
    "```\n",
    "\n",
    "But what if I want to look at more than 1 week? For 20 year's worth, the compressed size is 16.7 GB and the uncompressed size is 58.2 GB\n",
    "\n",
    "```\n",
    "20-years\n",
    "└── parquet\n",
    "    ├── part.0.parquet\n",
    "    ├── part.1.parquet\n",
    "    ├── part.2.parquet\n",
    "   ...\n",
    "    ├── part.1094.parquet\n",
    "```\n",
    "\n",
    "One solution, is to read and process this dataset into chunks:\n",
    "\n",
    "```python\n",
    "files = pathlib.Path(\"../data/timeseries/20-years/parquet/\").glob(\"part.*.parquet\")\n",
    "counts = pd.Series(dtype=int)\n",
    "for path in files:\n",
    "    # read in each chunk\n",
    "    df = pd.read_parquet(path)\n",
    "    # sum counts as you go, storing each chunk\n",
    "    counts = counts.add(df['name'].value_counts(), fill_value=0)\n",
    "    counts.astype(int)\n",
    "```\n",
    "\n",
    "But each of these operations doesn't need to happen serially, we could read the chunks and calculate the number of unique values in parallel, and sum at the end. The tricky part is figuring out how to break the computation into chunks, which in this example was fairly straightforward, but other computations can become more complicted, which is where Dask comes in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c27f662-78e0-4193-9ead-93608a5b66c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# What is Dask?\n",
    "\n",
    "Dask is an open-source parallel & distributed computing framework for Python, which supports __larger-than-memory computation__, enabling data processing and modeling for datasets that don’t fit in RAM. Dask is used in a wide range of domains from finance and retail to academia and life sciences (check out [this video](https://youtu.be/t_GRK4L-bnw) or [blog post](https://coiled.io/blog/who-uses-dask/) on who uses Dask). It is also leveraged internally by numerous special-purpose tools like XGBoost, RAPIDS, PyTorch, Prefect, and Airflow. It’s developed in ongoing collaboration with the PyData community making it easy to learn, integrate, and operate. The familiarity of collections like Dask DataFrame to pandas allow you to quickly get started on your hardware of choice– be it your laptop, cloud service, or HPC cluster.\n",
    "\n",
    "# How can I use Dask?\n",
    "\n",
    "If you are familiar with Numpy, pandas and scikit-learn then think of Dask as their faster cousin. Take this example comparing the syntax for a simple groupby using pandas versus Dask DataFrame:\n",
    "\n",
    "```python\n",
    "# pandas syntax                       # dask dataframe syntax\n",
    "import pandas as pd                   import dask.dataframe as dd\n",
    "df = pd.read_csv('2015-01-01.csv')    df = dd.read_csv('2015-*-*.csv')\n",
    "df.groupby('user_id').value.mean()    df.groupby('user_id').value.mean().compute()\n",
    "```\n",
    "\n",
    "# How does Dask work?\n",
    "\n",
    "We can think of Dask at a high- and low- level. The high-level colletions Array, Bag, and DataFrame mimic NumPy, lists, and pandas but can operate in parallel on datasets that don’t fit into memory. Delayed and Futures are low-level collections for custom computations. In addition to these high-level collections, Dask provides dynamic task schedulers that execute the task graphs created from high-level collections and custom workloads.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://raw.githubusercontent.com/dask/marketing/main/source/images/dask-overview.svg\" \n",
    "     width=\"70%\"\n",
    "     alt=\"Dask overview\">\n",
    "  <figcaption>In this workshop, we'll be focusing on the Dask DataFrame collection.</figcaption>\n",
    "</figure>\n",
    "\n",
    "# When should I use Dask?\n",
    "\n",
    "Do your data fit comfortably in memory and your computations are already blazingly fast? Love that for you! This means Dask *probably* isn't the right tool, though. You'll see the biggest performance improvements for problems that are memory-bound or compute-bound (or both). If you don't have any of these problems, you're likely better off using pandas or scikit-learn directly. If you're still not sure, check out [Why Dask?](https://docs.dask.org/en/latest/why.html)\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dask/dask-ml/main/docs/source/images/dimensions_of_scale.svg\"\n",
    "     width=\"40%\"\n",
    "     alt=\"Conceptual graph of size of the model vs. the size of the data.\"/>\n",
    "\n",
    "__Bottom Left:__ You don't need Dask.\n",
    "__Elsewhere:__ Dask is fair game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8795285-c7b0-4094-a9e5-6313758ca5ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# What we'll cover in the next 40 minutes\n",
    "\n",
    "- Load and process Seattle bicycle counter data using the Dask Delayed, Bag, and Dask DataFrame collections\n",
    "- (Time permitting) Forecast what the future of cycling might look like with Prophet\n",
    "- Monitor these computations using the interactive dashboard\n",
    "- Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beca61e0-0e54-449e-a7e0-bda38f4cbdcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b6fe04e-fde7-4823-8ac5-d230b784ce7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"../intro-slide-wids22.svg\"\n",
    "     max_width=100%\n",
    "     height=auto\n",
    "     alt=\"WiDS title slide\" />\n",
    "     \n",
    "Feel free to connect with me on LinkedIn: https://www.linkedin.com/in/scharlottej13/ or on Twitter @scharlottej13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414d9eeb-6de9-40e6-a59b-0601c254f40c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!---\n",
    "OUTLINE\n",
    "\n",
    "go over the intro\n",
    "dive into some quick syntax, like pull in the data, run some models, just give people a feel for it\n",
    "then go into some quick concepts/definitions\n",
    "run THE SAME EXACT CODE again, but this time pull up the dashboard and have it going at the same time\n",
    "-->\n",
    "\n",
    "<center>\n",
    "  <img src=\"https://raw.githubusercontent.com/dask/marketing/main/source/images/dask-horizontal.svg\" alt=\"Dask logo\" width=55%/>\n",
    "</center>\n",
    "\n",
    "# Materials and setup\n",
    "\n",
    "Tutorial materials available at https://github.com/coiled/[]. Two ways to go through the tutorial:\n",
    "- Run using Binder (no setup required, recommended for today!)\n",
    "- Run locally on your laptop (recommended for going back through materials later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d7de6e-7235-4422-b346-dfaa5a9a3801",
   "metadata": {},
   "source": [
    "# A common problem\n",
    "\n",
    "Timseries data going back 20 years with one row per second (publicly accessible on https://github.com/coiled/coiled-datasets). Using pandas, it's very easy to take one week of this dataset and calculate the counts of unique values for a given column:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_parquet(\"s3://coiled-datasets/timeseries/20-years/parquet/part.0.parquet\")\n",
    "df['name'].value_counts().astype(int)\n",
    "```\n",
    "\n",
    "But what if I want to look at more than 1 week? For 20 year's worth, the compressed size is 16.7 GB and the uncompressed size is 58.2 GB\n",
    "\n",
    "```\n",
    "20-years\n",
    "└── parquet\n",
    "    ├── part.0.parquet\n",
    "    ├── part.1.parquet\n",
    "    ├── part.2.parquet\n",
    "   ...\n",
    "    ├── part.1094.parquet\n",
    "```\n",
    "\n",
    "One solution, is to read and process this dataset into chunks:\n",
    "\n",
    "```python\n",
    "files = pathlib.Path(\"../data/timeseries/20-years/parquet/\").glob(\"part.*.parquet\")\n",
    "counts = pd.Series(dtype=int)\n",
    "for path in files:\n",
    "    # read in each chunk\n",
    "    df = pd.read_parquet(path)\n",
    "    # sum counts as you go, storing each chunk\n",
    "    counts = counts.add(df['name'].value_counts(), fill_value=0)\n",
    "    counts.astype(int)\n",
    "```\n",
    "\n",
    "But each of these operations doesn't need to happen serially, we could read the chunks and calculate the number of unique values in parallel, and sum at the end. The tricky part is figuring out how to break the computation into chunks, which in this example was fairly straightforward, but other computations can become more complicted, which is where Dask comes in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c27f662-78e0-4193-9ead-93608a5b66c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# What is Dask?\n",
    "\n",
    "Dask is an open-source parallel & distributed computing framework for Python, which supports __larger-than-memory computation__, enabling data processing and modeling for datasets that don’t fit in RAM. Dask is used in a wide range of domains from finance and retail to academia and life sciences (check out [this video](https://youtu.be/t_GRK4L-bnw) or [blog post](https://coiled.io/blog/who-uses-dask/) on who uses Dask). It is also leveraged internally by numerous special-purpose tools like XGBoost, RAPIDS, PyTorch, Prefect, and Airflow. It’s developed in ongoing collaboration with the PyData community making it easy to learn, integrate, and operate. The familiarity of collections like Dask DataFrame to pandas allow you to quickly get started on your hardware of choice– be it your laptop, cloud service, or HPC cluster.\n",
    "\n",
    "# How can I use Dask?\n",
    "\n",
    "If you are familiar with Numpy, pandas and scikit-learn then think of Dask as their faster cousin. Take this example comparing the syntax for a simple groupby using pandas versus Dask dataframe:\n",
    "\n",
    "```python\n",
    "# pandas syntax                       # dask dataframe syntax\n",
    "import pandas as pd                   import dask.dataframe as dd\n",
    "df = pd.read_csv('2015-01-01.csv')    df = dd.read_csv('2015-*-*.csv')\n",
    "df.groupby(df.user_id).value.mean()   df.groupby(df.user_id).value.mean().compute()\n",
    "```\n",
    "\n",
    "# How does Dask work?\n",
    "\n",
    "We can think of Dask at a high- and low- level. The high-level colletions Array, Bag, and DataFrame mimic NumPy, lists, and pandas but can operate in parallel on datasets that don’t fit into memory. Delayed and Futures are low-level collections for custom computations. In addition to these high-level collections, Dask provides dynamic task schedulers that execute the task graphs created from high-level collections and custom workloads.\n",
    "\n",
    "<!-- https://towardsdatascience.com/the-beginners-guide-to-distributed-computing-6d6833796318 -->\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://raw.githubusercontent.com/dask/marketing/main/source/images/dask-overview.svg\" \n",
    "     width=\"50%\"\n",
    "     alt=\"Dask overview\">\n",
    "  <figcaption>In this workshop, we'll be focusing on the Dask DataFrame collection.</figcaption>\n",
    "</figure>\n",
    "\n",
    "# When should I use Dask?\n",
    "\n",
    "Do your data fit comfortably in memory and your computations are already blazingly fast? Love that for you! This means Dask *probably* isn't the right tool, though. You'll see the biggest performance improvements for problems that are memory-bound or compute-bound (or both). If you don't have any of these problems, you're likely better off using pandas or scikit-learn directly. If you're still not sure, check out [Why Dask?](https://docs.dask.org/en/latest/why.html)\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dask/dask-ml/main/docs/source/images/dimensions_of_scale.svg\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Conceptual graph of size of the model vs. the size of the data.\"/>\n",
    "\n",
    "__Bottom Left:__ You don't need Dask.\n",
    "__Elsewhere:__ Dask is fair game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746c7d65-1639-4bb9-b481-0c959c910272",
   "metadata": {},
   "source": [
    "# Quick demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dd12ab-7674-407c-a5bc-9e6ec4111407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.linear_model import LogisticRegression\n",
    "from dask_ml.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1_000, chunks=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f62e1dc-8db4-4740-aff1-403c3329367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c917ece-a96b-4ebc-922a-9288b6220e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77361894-44df-414f-a630-63147367643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45328f38-d850-4bda-89a1-bb99d2e893d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb58e813-4295-441a-b6c9-1b146b2b8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictions = lr.predict(X).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefee8b7-b397-4ace-bf10-ad1498fec5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X,y).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8795285-c7b0-4094-a9e5-6313758ca5ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# What we'll cover in the next XX minutes\n",
    "\n",
    "- Load and process Seattle bicycle counter data using the Dask Bag and Dask DataFrame collections\n",
    "- Run a simple genearalized linear model \n",
    "- Monitor these computations using the interactive dashboard\n",
    "- Q&A\n",
    "\n",
    "# Resources\n",
    "\n",
    "Dask provides a friendly Python interface and visibility into computation via its dashboards and performance reports, but it can still be intimidating at first. Luckily, the [Dask docs](https://docs.dask.org) are extensive and have numerous [examples](https://examples.dask.org/) and [best practices](https://docs.dask.org/en/latest/best-practices.html). Plus, there is a welcoming community available to answer your questions on the [Dask Discourse](https://dask.discourse.group/)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
